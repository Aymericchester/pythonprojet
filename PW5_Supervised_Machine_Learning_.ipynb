{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWs_QwiYTu9O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PW5-Supervised Machine Learning :**\n",
        "\n",
        "**1 Summary**\n",
        "\n",
        "Decision trees are versatile Machine learning algorithms able to perform both classification and regression tasks.\n",
        "There are able to to fit complex data. There are the fundamental components of random forests which are among\n",
        "the most powerful Machien Learning algorithms available today. In this PW we start with discussing how to train,\n",
        "visualize and make prediction with simple decision trees.\n",
        "\n",
        "**2 Exercises**\n",
        "\n",
        "**Exercise 1 : classification of Iris Images**\n",
        "\n",
        "In this exercise we start with a simple dataset widely used in the literature and well known by ChatGPT. Never-\n",
        "theless, you will have time to do it by your self. The famous dataset of Iris contains the sepal and petal length and\n",
        "width of 150 iris flowers of three different species: Iris Setosa, Iris versicolor, and Iris virginica (Iris Dataset). As\n",
        "you can learn from this dataset description, we have three classes of Iris and we propose a decision tree model to\n",
        "train and predict new Iris.\n",
        "\n",
        "1. load from sklean.datasets the Iris dataset after importing load iris data object and use a variable-name iris\n",
        "to store the returned value: iris = load iris();\n"
      ],
      "metadata": {
        "id": "wuz8m4x7UG1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()"
      ],
      "metadata": {
        "id": "WVRG_iSRUy36"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. extract from iris.data the petal length and width then affect them to an input data as a new variable X. Note\n",
        "that the different columns or features of iris are the following:\n",
        "\n",
        "• Length of the sepal (in cm)\n",
        "\n",
        "• Width of the sepal (in cm)\n",
        "\n",
        "• Length of the petal (in cm)\n",
        "\n",
        "• Width of the petal (in cm)"
      ],
      "metadata": {
        "id": "n5U4uRlEV2Qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "petal_length_width = iris.data[:, 2:]  # Extracting columns 2 and 3 (indexing starts from 0)\n",
        "\n",
        "# Creating a new variable X to store petal length and width\n",
        "X = petal_length_width\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GN0NCIrV8gC",
        "outputId": "28cc46c1-cbea-40ae-fcb0-03372fad3c2d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.4 0.2]\n",
            " [1.4 0.2]\n",
            " [1.3 0.2]\n",
            " [1.5 0.2]\n",
            " [1.4 0.2]\n",
            " [1.7 0.4]\n",
            " [1.4 0.3]\n",
            " [1.5 0.2]\n",
            " [1.4 0.2]\n",
            " [1.5 0.1]\n",
            " [1.5 0.2]\n",
            " [1.6 0.2]\n",
            " [1.4 0.1]\n",
            " [1.1 0.1]\n",
            " [1.2 0.2]\n",
            " [1.5 0.4]\n",
            " [1.3 0.4]\n",
            " [1.4 0.3]\n",
            " [1.7 0.3]\n",
            " [1.5 0.3]\n",
            " [1.7 0.2]\n",
            " [1.5 0.4]\n",
            " [1.  0.2]\n",
            " [1.7 0.5]\n",
            " [1.9 0.2]\n",
            " [1.6 0.2]\n",
            " [1.6 0.4]\n",
            " [1.5 0.2]\n",
            " [1.4 0.2]\n",
            " [1.6 0.2]\n",
            " [1.6 0.2]\n",
            " [1.5 0.4]\n",
            " [1.5 0.1]\n",
            " [1.4 0.2]\n",
            " [1.5 0.2]\n",
            " [1.2 0.2]\n",
            " [1.3 0.2]\n",
            " [1.4 0.1]\n",
            " [1.3 0.2]\n",
            " [1.5 0.2]\n",
            " [1.3 0.3]\n",
            " [1.3 0.3]\n",
            " [1.3 0.2]\n",
            " [1.6 0.6]\n",
            " [1.9 0.4]\n",
            " [1.4 0.3]\n",
            " [1.6 0.2]\n",
            " [1.4 0.2]\n",
            " [1.5 0.2]\n",
            " [1.4 0.2]\n",
            " [4.7 1.4]\n",
            " [4.5 1.5]\n",
            " [4.9 1.5]\n",
            " [4.  1.3]\n",
            " [4.6 1.5]\n",
            " [4.5 1.3]\n",
            " [4.7 1.6]\n",
            " [3.3 1. ]\n",
            " [4.6 1.3]\n",
            " [3.9 1.4]\n",
            " [3.5 1. ]\n",
            " [4.2 1.5]\n",
            " [4.  1. ]\n",
            " [4.7 1.4]\n",
            " [3.6 1.3]\n",
            " [4.4 1.4]\n",
            " [4.5 1.5]\n",
            " [4.1 1. ]\n",
            " [4.5 1.5]\n",
            " [3.9 1.1]\n",
            " [4.8 1.8]\n",
            " [4.  1.3]\n",
            " [4.9 1.5]\n",
            " [4.7 1.2]\n",
            " [4.3 1.3]\n",
            " [4.4 1.4]\n",
            " [4.8 1.4]\n",
            " [5.  1.7]\n",
            " [4.5 1.5]\n",
            " [3.5 1. ]\n",
            " [3.8 1.1]\n",
            " [3.7 1. ]\n",
            " [3.9 1.2]\n",
            " [5.1 1.6]\n",
            " [4.5 1.5]\n",
            " [4.5 1.6]\n",
            " [4.7 1.5]\n",
            " [4.4 1.3]\n",
            " [4.1 1.3]\n",
            " [4.  1.3]\n",
            " [4.4 1.2]\n",
            " [4.6 1.4]\n",
            " [4.  1.2]\n",
            " [3.3 1. ]\n",
            " [4.2 1.3]\n",
            " [4.2 1.2]\n",
            " [4.2 1.3]\n",
            " [4.3 1.3]\n",
            " [3.  1.1]\n",
            " [4.1 1.3]\n",
            " [6.  2.5]\n",
            " [5.1 1.9]\n",
            " [5.9 2.1]\n",
            " [5.6 1.8]\n",
            " [5.8 2.2]\n",
            " [6.6 2.1]\n",
            " [4.5 1.7]\n",
            " [6.3 1.8]\n",
            " [5.8 1.8]\n",
            " [6.1 2.5]\n",
            " [5.1 2. ]\n",
            " [5.3 1.9]\n",
            " [5.5 2.1]\n",
            " [5.  2. ]\n",
            " [5.1 2.4]\n",
            " [5.3 2.3]\n",
            " [5.5 1.8]\n",
            " [6.7 2.2]\n",
            " [6.9 2.3]\n",
            " [5.  1.5]\n",
            " [5.7 2.3]\n",
            " [4.9 2. ]\n",
            " [6.7 2. ]\n",
            " [4.9 1.8]\n",
            " [5.7 2.1]\n",
            " [6.  1.8]\n",
            " [4.8 1.8]\n",
            " [4.9 1.8]\n",
            " [5.6 2.1]\n",
            " [5.8 1.6]\n",
            " [6.1 1.9]\n",
            " [6.4 2. ]\n",
            " [5.6 2.2]\n",
            " [5.1 1.5]\n",
            " [5.6 1.4]\n",
            " [6.1 2.3]\n",
            " [5.6 2.4]\n",
            " [5.5 1.8]\n",
            " [4.8 1.8]\n",
            " [5.4 2.1]\n",
            " [5.6 2.4]\n",
            " [5.1 2.3]\n",
            " [5.1 1.9]\n",
            " [5.9 2.3]\n",
            " [5.7 2.5]\n",
            " [5.2 2.3]\n",
            " [5.  1.9]\n",
            " [5.2 2. ]\n",
            " [5.4 2.3]\n",
            " [5.1 1.8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. print the target variable’s names, values and count the number of classes"
      ],
      "metadata": {
        "id": "pvtczt4mXK5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Target variable's names\n",
        "target_names = iris.target_names\n",
        "print(\"Target Variable Names:\", target_names)\n",
        "\n",
        "# Target variable's values\n",
        "target_values = iris.target\n",
        "print(\"Target Variable Values:\", target_values)\n",
        "\n",
        "# Count the number of classes\n",
        "num_classes = len(set(target_values))\n",
        "print(\"Number of Classes:\", num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es-d3M66XO82",
        "outputId": "09f2cb45-563b-423f-8dd1-ba9f70225928"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target Variable Names: ['setosa' 'versicolor' 'virginica']\n",
            "Target Variable Values: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n",
            "Number of Classes: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Import DecisionTreeClassifier from sklearn.tree and create a new object treeClassifer with a maximum\n",
        "depth value equal to 2"
      ],
      "metadata": {
        "id": "F4gJtS3YXu8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Creating a DecisionTreeClassifier object with a maximum depth of, for example, 2\n",
        "max_depth_value = 2  # You can change this value as needed\n",
        "treeClassifier = DecisionTreeClassifier(max_depth=max_depth_value)"
      ],
      "metadata": {
        "id": "xhUQribgYFdm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "max depth decision tree\n",
        "\n",
        "• The max depth parameter is a key hyperparameter in decision trees. It controls the maximum\n",
        "depth of the tree corresponding to the maximum number of levels the tree is allowed to have,\n",
        "counting from the root node to the leaf nodes. Then the decision tree construction algorithm\n",
        "stops splitting nodes beyond this limit. Thus, each path from the root to a leaf cannot exceed\n",
        "this depth.\n",
        "\n",
        "• Limiting tree depth is a common technique for avoiding over-fitting in decision tree models. An\n",
        "unconstrained decision tree can become very complex and adapt very strongly to the training\n",
        "data, which can prevent it from generalizing well to new data.\n",
        "\n",
        "• Bias-Variance Tradeoff: Bias-variance trade-off: by controlling tree growth, max depth can\n",
        "contribute to finding a balance between bias and variance. A tree that is too shallow risks being\n",
        "under-adjusted (high bias, low variance), while one that is too deep risks being over-adjusted\n",
        "(low bias, high variance). Choosing the optimum depth strikes a balance between the two.\n",
        "\n",
        "• it is often advisable to adjust max depth using cross-validation or another hyperparameter op-\n",
        "timizer to ensure optimal model performance on a validation dataset.\n",
        "\n",
        "5. fit your decision tree on your data"
      ],
      "metadata": {
        "id": "TRxxh1jhYWgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the DecisionTreeClassifier on the data (X) and target variable (iris.target)\n",
        "treeClassifier.fit(X, iris.target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "rm26cccpZSZH",
        "outputId": "4aeb28a5-beac-4cfc-fe36-b7222d6e9d40"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(max_depth=2)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=2)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. visualize your decision tree using ”export graphviz()”.This method provides a graph definition file in a\n",
        "”.dot” format. Name this file ”Iris DTree.dot”. Below, you find the core definition of the function ”ex-\n",
        "port graphviz()”"
      ],
      "metadata": {
        "id": "nXfMTIZCaIeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imports the export_graphviz function from the Scikit-learn library, which is used to generate a GraphViz representation of the decision tree.\n",
        "from sklearn.tree import export_graphviz\n",
        "#imports the Image class from IPython’s display module, which can be used to display images in a Jupyter Notebook or IPython environment.\n",
        "from IPython.display import Image\n",
        "#generates a GraphViz representation of the decision tree called \"dot\".\n",
        "dot=export_graphviz(treeClassifier,\n",
        "  out_file=\"Iris_DTree.dot\", #specifies the output file name for the GraphViz representation. In this case, the tree will be saved as a file named \"Iris_DTree.dot\".\n",
        "  feature_names=iris.feature_names[2:], #specifies the feature names for the tree visualization. It uses the feature names starting from the third feature (index 2) of the Iris dataset.\n",
        "  class_names=target_names, #specifies the class names for the tree visualization.\n",
        "  rounded=True, #is an additional option to round the nodes and fill them with colors based on class.\n",
        "  filled=True); #option"
      ],
      "metadata": {
        "id": "wzM0D3dKaNxb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code generates a GraphViz representation of the decision tree and saves it as a .dot file named ”Iris DTree.dot”\n",
        "in the current directory. To visualize the tree, you might need to convert this .dot file to an image format\n",
        "using GraphViz software.\n",
        "\n",
        "7. call the command online as following:"
      ],
      "metadata": {
        "id": "4I0pyjSSbeuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!It allows executing shell commands directly from the notebook.\n",
        "!dot -Tpng Iris_DTree.dot -o Iris_DTree.png\n",
        "#-T: type; -o: output"
      ],
      "metadata": {
        "id": "lxmxOIyRbiYH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this command is executed in a Jupyter Notebook cell with the character ”!” as a prefix. It takes the ”.dot”\n",
        "file (Iris DTree.dot) as input, converts it to a PNG image using the dot tool, and saves the resulting image\n",
        "as Iris DTree.png. This enables visualization of the decision tree in a graphical format. Normaly you have\n",
        "the image below displayed on your Notebook\n",
        "\n",
        "**Exercise 2: Decision Tree Interpretation**\n",
        "\n",
        "From the graph obtained on the Iris dataset, we obtained a tree with a level of deep equal to 2 when we include\n",
        "the root as a starting level (level of root equal to 0 and the last level of leafs is equal to 2). If we take any node in\n",
        "the tree, it contains the following information:\n",
        "\n",
        "• petal width (cm) ≤ 0.8: corresponds to the petal feature selected based on Gini criteria.\n",
        "\n",
        "• Gini criteria: it measure the impurity of a node according a selected feature. Indeed the Gini criteria is a\n",
        "metric in 0-1 range and calculated based on the number of samples having the same label according to the\n",
        "selected label.\n",
        "\n",
        "• Samples: is the number of samples verifying the Gini criteria.\n",
        "\n",
        "• Value: is vector of shape equal to (1, classes) Class: is the label affected to the node and corresponds to the\n",
        "major label of the samples belonging to this class\n",
        "\n",
        "1. calculate the Gini value for both features and verify that the petal length feature is well selected as a root\n",
        "node. We recall the Gini impurity formula here: Ginii = 1 − Pn\n",
        "k=1 p2\n",
        "i,k where pi,k is the ratio of class k\n",
        "instances among the training instances in the ith node. The gini value of the root is: 1 − 3 × (50/150)2 =\n",
        "0.667"
      ],
      "metadata": {
        "id": "yJ5PFTRibrv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(target_values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU-HIha0eSnP",
        "outputId": "f85763ac-a9a7-45ef-e420-32c3961f8249"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0, 1, 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_gini_impurity(feature_values, target_values):\n",
        "    classes = set(target_values)\n",
        "    total_samples = len(target_values)\n",
        "    gini = 0.667\n",
        "    for value in feature_values:\n",
        "        samples_left = [target_values[i] for i, val in enumerate(feature_values) if val <= value]\n",
        "        samples_right = [target_values[i] for i, val in enumerate(feature_values) if val > value]\n",
        "\n",
        "        # Calculate Gini impurity for left and right nodes\n",
        "        if len(samples_left)==0:\n",
        "          gini_left+=0\n",
        "        else:\n",
        "          gini_left = 1-sum((samples_left.count(c) / len(samples_left))**2 for c in classes)\n",
        "        if len(samples_right)==0:\n",
        "          gini_right +=0\n",
        "        else:\n",
        "          gini_right = 1-sum((samples_right.count(c) / len(samples_right))**2 for c in classes)\n",
        "\n",
        "        # Calculate weighted Gini impurity\n",
        "        weighted_gini = (len(samples_left) / total_samples) * gini_left + (len(samples_right) / total_samples) * gini_right\n",
        "\n",
        "        # Update overall Gini impurity\n",
        "        gini = min(gini, weighted_gini) if gini != 0 else weighted_gini\n",
        "\n",
        "    return gini\n",
        "\n",
        "# Petal length and petal width values from the Iris dataset\n",
        "petal_length = X[:, 0]  # Assuming petal length is the first feature\n",
        "petal_width = X[:, 1]   # Assuming petal width is the second feature\n",
        "\n",
        "# Target values (labels)\n",
        "target_values = iris.target\n",
        "\n",
        "# Calculate Gini impurity for petal length and petal width features\n",
        "gini_petal_length = calculate_gini_impurity(petal_length, target_values)\n",
        "gini_petal_width = calculate_gini_impurity(petal_width, target_values)\n",
        "\n",
        "# Print the calculated Gini impurity for both features\n",
        "print(\"Gini Impurity for Petal Length:\", gini_petal_length)\n",
        "print(\"Gini Impurity for Petal Width:\", gini_petal_width)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdFV2uwcbu7H",
        "outputId": "228ce62f-5718-4eaa-8314-3b164ebb81d6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gini Impurity for Petal Length: 0.3333333333333333\n",
            "Gini Impurity for Petal Width: 0.3333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. A decision tree can also estimate the probability that an instance belongs to a particular class k. Suppose\n",
        "you hve found a flower whose petals are 5 cm long and 1.5 cm on width. Use the tree to estimate the\n",
        "probability to belong to each class. Note: you can use the function predict proba()."
      ],
      "metadata": {
        "id": "qCVTVqUGfjIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flower's petal length and width\n",
        "flower_petal_length = 5.0\n",
        "flower_petal_width = 1.5\n",
        "\n",
        "# Create a new data point for prediction using the flower's petal length and width\n",
        "new_data_point = [[flower_petal_length, flower_petal_width]]\n",
        "\n",
        "# Use the trained decision tree model to predict probabilities for the new data point\n",
        "predicted_probabilities = treeClassifier.predict_proba(new_data_point)\n",
        "\n",
        "# Display the estimated probabilities for each class\n",
        "print(\"Estimated Probabilities for Each Class:\")\n",
        "for i, class_name in enumerate(iris.target_names):\n",
        "    print(f\"{class_name}: {predicted_probabilities[0][i]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp9Q2g62fj3f",
        "outputId": "13f4a4df-f4f2-47e2-a702-76b3dab8cd8c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Probabilities for Each Class:\n",
            "setosa: 0.0000\n",
            "versicolor: 0.9074\n",
            "virginica: 0.0926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. interpreat the probabilities estimated before using the tree graph to extract the path. Rouhphly speaking,\n",
        "you have to start asking the root node whether the petal length of the flower is smaller or greater than 0.8.\n",
        "Depending on the aswer you will move down to the root’s left child or to the right one. You have to do\n",
        "the same process until you reach the final leaf. The final sheet will be the one with the highest estimated\n",
        "probability amongst the 3."
      ],
      "metadata": {
        "id": "Fq84s4Shf62V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Given new data point\n",
        "new_data_point = [[5.0, 1.5]]  # Replace with actual petal length and width values\n",
        "\n",
        "# Get the predicted probabilities for the new data point\n",
        "predicted_probabilities = treeClassifier.predict_proba(new_data_point)\n",
        "\n",
        "# Traverse the decision tree to extract the path\n",
        "node_index = 0  # Start from the root node (index 0)\n",
        "while True:\n",
        "    feature_index = treeClassifier.tree_.feature[node_index]\n",
        "    threshold = treeClassifier.tree_.threshold[node_index]\n",
        "\n",
        "    print(f\"Node {node_index}:\")\n",
        "    if feature_index != -2:  # If not a leaf node\n",
        "        if new_data_point[0][feature_index] <= threshold:\n",
        "            print(f\"  Go to left child (feature {feature_index} <= {threshold})\")\n",
        "            node_index = treeClassifier.tree_.children_left[node_index]\n",
        "        else:\n",
        "            print(f\"  Go to right child (feature {feature_index} > {threshold})\")\n",
        "            node_index = treeClassifier.tree_.children_right[node_index]\n",
        "    else:\n",
        "        # Reached a leaf node, print predicted probabilities and break the loop\n",
        "        print(f\"  Leaf node. Predicted Probabilities:\")\n",
        "        for i, class_name in enumerate(iris.target_names):\n",
        "            print(f\"    {class_name}: {predicted_probabilities[0][i]:.4f}\")\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOjknbqbf9iy",
        "outputId": "d1e23c3d-8836-4284-92f7-39f16809e2a4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node 0:\n",
            "  Go to right child (feature 1 > 0.800000011920929)\n",
            "Node 2:\n",
            "  Go to left child (feature 1 <= 1.75)\n",
            "Node 3:\n",
            "  Leaf node. Predicted Probabilities:\n",
            "    setosa: 0.0000\n",
            "    versicolor: 0.9074\n",
            "    virginica: 0.0926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Verify the last result using the function predict."
      ],
      "metadata": {
        "id": "ln852k26g4oS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Given new data point\n",
        "new_data_point = [[5.0, 1.5]]  # Replace with actual petal length and width values\n",
        "\n",
        "# Get the predicted class using the predict() function\n",
        "predicted_class = treeClassifier.predict(new_data_point)\n",
        "\n",
        "# Display the predicted class obtained from predict() function\n",
        "print(\"Predicted Class using predict() function:\", iris.target_names[predicted_class[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLtmdamXhAl1",
        "outputId": "db4948b2-3c6b-4917-97b7-2813de97122a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class using predict() function: versicolor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 4: k-fold cross validation**\n",
        "\n",
        "Now we are interesting to evaluate the performance of our tree by splitting data into training and test. We introduce\n",
        "in this exercise the k-fold cross validation technique where a dataset is divided into k subsets or folds. The model\n",
        "is trained and evaluated k times, using a different fold as the validation set each time. Performance metrics from\n",
        "each fold are averaged to estimate the model’s generalization performance. Here is a simple implementation of\n",
        "the k-fold cross-validation The algorithm is the following"
      ],
      "metadata": {
        "id": "Gv40I0hLhKky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "def kfoldCrossValidation(X,y,M):\n",
        "# Set up k-fold cross-validation\n",
        "  kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "  scores = []\n",
        "\n",
        " # Perform k-fold cross-validation\n",
        "  for train_index, test_index in kfold.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "  # Fit the classifier on the training data\n",
        "    M.fit(X_train, y_train)\n",
        "\n",
        "   # Predict on the test data\n",
        "    y_pred = M.predict(X_test)\n",
        "\n",
        "  # Calculate accuracy and store in scores list\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    scores.append(accuracy)\n",
        "  return scores"
      ],
      "metadata": {
        "id": "M6eVWhuqhytS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Modify the function kfoldCrossValidation after including the k and the machine learning model M, as\n",
        "additional input parameters"
      ],
      "metadata": {
        "id": "eiw4oGaniSV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def kfoldCrossValidation(X, y, k, M):\n",
        "    # Set up k-fold cross-validation\n",
        "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "    scores = []\n",
        "\n",
        "    # Perform k-fold cross-validation\n",
        "    for train_index, test_index in kfold.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        # Fit the classifier on the training data\n",
        "        M.fit(X_train, y_train)\n",
        "\n",
        "        # Predict on the test data\n",
        "        y_pred = M.predict(X_test)\n",
        "\n",
        "        # Calculate accuracy and store in scores list\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        scores.append(accuracy)\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "JGMLLUWGiTKG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. implement the average accuracy from the scores array returned by the function kfoldCrossValidation"
      ],
      "metadata": {
        "id": "WMuQ0N8ni_lR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = kfoldCrossValidation(X, y, 2, M)\n",
        "average_accuracy = sum(scores) / len(scores)\n",
        "\n",
        "# Display the average accuracy\n",
        "print(\"Average Accuracy:\", average_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "aLFYJHRWjXhF",
        "outputId": "acde33ca-50ef-43a3-f1e0-cb811418bc74"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-ad88853c2735>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfoldCrossValidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maverage_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Display the average accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
          ]
        }
      ]
    }
  ]
}